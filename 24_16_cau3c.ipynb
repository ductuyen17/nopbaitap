{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     accept  admin  again  ai_bảo  all  amstel  and  anh  anh_chị  anh_em  \\\n",
      "0       0.0    0.0    0.0     0.0  0.0     0.0  0.0  0.0      0.0     0.0   \n",
      "1       0.0    0.0    0.0     0.0  0.0     0.0  0.0  0.0      0.0     0.0   \n",
      "2       0.0    0.0    0.0     0.0  0.0     0.0  0.0  0.0      0.0     0.0   \n",
      "3       0.0    0.0    0.0     0.0  0.0     0.0  0.0  0.0      0.0     0.0   \n",
      "4       0.0    0.0    0.0     0.0  0.0     0.0  0.0  0.0      0.0     0.0   \n",
      "..      ...    ...    ...     ...  ...     ...  ...  ...      ...     ...   \n",
      "592     0.0    0.0    0.0     0.0  0.0     0.0  0.0  0.0      0.0     0.0   \n",
      "593     0.0    0.0    0.0     0.0  0.0     0.0  0.0  0.0      0.0     0.0   \n",
      "594     0.0    0.0    0.0     0.0  0.0     0.0  0.0  0.0      0.0     0.0   \n",
      "595     0.0    0.0    0.0     0.0  0.0     0.0  0.0  0.0      0.0     0.0   \n",
      "596     0.0    0.0    0.0     0.0  0.0     0.0  0.0  0.0      0.0     0.0   \n",
      "\n",
      "     ...  ống  ồn_ào   ổg       ổn  ổnquán   ới   ớn  ớncầm   ớt  ức_chế  \n",
      "0    ...  0.0    0.0  0.0  0.00000     0.0  0.0  0.0    0.0  0.0     0.0  \n",
      "1    ...  0.0    0.0  0.0  0.00000     0.0  0.0  0.0    0.0  0.0     0.0  \n",
      "2    ...  0.0    0.0  0.0  0.00000     0.0  0.0  0.0    0.0  0.0     0.0  \n",
      "3    ...  0.0    0.0  0.0  0.24979     0.0  0.0  0.0    0.0  0.0     0.0  \n",
      "4    ...  0.0    0.0  0.0  0.00000     0.0  0.0  0.0    0.0  0.0     0.0  \n",
      "..   ...  ...    ...  ...      ...     ...  ...  ...    ...  ...     ...  \n",
      "592  ...  0.0    0.0  0.0  0.00000     0.0  0.0  0.0    0.0  0.0     0.0  \n",
      "593  ...  0.0    0.0  0.0  0.00000     0.0  0.0  0.0    0.0  0.0     0.0  \n",
      "594  ...  0.0    0.0  0.0  0.00000     0.0  0.0  0.0    0.0  0.0     0.0  \n",
      "595  ...  0.0    0.0  0.0  0.00000     0.0  0.0  0.0    0.0  0.0     0.0  \n",
      "596  ...  0.0    0.0  0.0  0.00000     0.0  0.0  0.0    0.0  0.0     0.0  \n",
      "\n",
      "[597 rows x 2199 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import emoji\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tu_dien = {}\n",
    "file_path = 'D:/khai pha web/finallabc4/chuanhoa.txt' \n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        word, abbreviation = line.strip().split('\t')\n",
    "        tu_dien[word.strip()] = abbreviation.strip()\n",
    "f = open(\"D:/khai pha web/finallabc4/vietnamese-stopwords.txt\", \"r\", encoding=\"utf-8\")\n",
    "#Get Stop words Dictionaries\n",
    "List_StopWords=f.read().split(\"\\n\")\n",
    "src='C:/Users/ADMIN/Documents/data6/'\n",
    "path=os.listdir(src)\n",
    "line=\"\"\n",
    "for i in path:\n",
    "    label=i.split(\"'\")[0]\n",
    "    f=open('C:/Users/ADMIN/Documents/data6/'+str(label), \"r\", encoding=\"utf-8\")\n",
    "    text=f.read()\n",
    "    # Loại bỏ thẻ HTML\n",
    "    mau_url = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    # Thay thế tất cả các URL trong chuỗi bằng chuỗi rỗng\n",
    "    text_pre = re.sub(mau_url, '', text)\n",
    "    text_pre=emoji.demojize(text_pre)\n",
    "    text_pre=text_pre.lower() # Convert text to lowercase\n",
    "    text_pre=re.sub(r'[^\\w\\s.]','',text_pre) # Remove punctuation\n",
    "    text_pre = re.sub(\"\\d+\", \" \", text_pre) # Remove number\n",
    "    text_pre=text_pre.replace(\"\\n\",\"\")  # Remove the newline command\n",
    "    text_pre = \" \".join(text_pre for text_pre in text_pre.split() if text_pre not in List_StopWords)\n",
    "    text_pre = re.sub(r\"[!@#$[]()]'\", \"\", text_pre) # Remove character: !@#$[]()\n",
    "# Get Abbreviations Words\n",
    "    text_pree=\"\"\n",
    "    words = text_pre.split()\n",
    "    for word in words:\n",
    "        w=word\n",
    "        w = re.sub(r'[^\\w\\s]','',w) #Removing Punctuation\n",
    "        if w.lower() in tu_dien:\n",
    "            word=tu_dien[w]\n",
    "        text_pree=text_pree + \" \" + word \n",
    "    line+=text_pree\n",
    "Dataa=line.split(' . ')\n",
    "Data=[]\n",
    "for x in Dataa:\n",
    "    if x.strip() != '':\n",
    "        Data.append(x)\n",
    "\n",
    "tr_idf_model  = TfidfVectorizer()\n",
    "tf_idf_vector = tr_idf_model.fit_transform(Data)\n",
    "\n",
    "tf_idf_array = tf_idf_vector.toarray()\n",
    "# print(tf_idf_array)\n",
    "\n",
    "W = tr_idf_model.get_feature_names_out()\n",
    "# print(W)\n",
    "\n",
    "df_tf_idf = pd.DataFrame(tf_idf_array, columns = W)\n",
    "print(df_tf_idf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
